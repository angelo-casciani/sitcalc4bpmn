import argparse
import datetime
import os
import sys
import csv
from typing import List, Dict, Optional
import time
import re

eval_src_dir = os.path.dirname(os.path.abspath(__file__))
main_src_dir = os.path.abspath(os.path.join(eval_src_dir, '..', '..', 'src'))
sys.path.insert(0, eval_src_dir)
sys.path.insert(0, main_src_dir)

from metrics_collector import MetricsCollector, ReasoningMetrics
from translator_service import TranslatorService
from reasoning_service import ReasoningService
from chart_generator import ChartGenerator


class BPMNEvaluator:
    def __init__(self, dataset_dir: str, output_dir: str, resume_csv: str = None):
        self.dataset_dir = dataset_dir
        self.output_dir = output_dir
        self.results = []
        # (Translations are performed when evaluating models; translation metrics are generated by the dedicated script.)
        self.current_datetime_str = None  # Store timestamp for file naming
        self.resume_csv = resume_csv  # Path to CSV file for resuming
        self.evaluated_samples = set()  # Track already evaluated sample IDs
        self.current_results_file = None  # Current CSV file path for incremental saving
        # Add 'status' and 'error_msg' to better distinguish failures
        self.csv_fieldnames = ['model_name', 'task_type', 'sample_id', 'expected', 'returned', 'correct', 'reasoning_time', 'inferences', 'status', 'error_msg']
        os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        self.pl_models_dir = os.path.join(self.project_root, 'pl_models', 'evaluation_temp')
        os.makedirs(self.pl_models_dir, exist_ok=True)
        self.translator = TranslatorService(output_dir=self.pl_models_dir)
        # Maximum error message length to store in CSVs; messages are sanitized to avoid breaking CSVs
        self.max_error_msg_len = 200

    def _sanitize_error_msg(self, msg: Optional[str]) -> str:
        """Sanitize a message for safe CSV storage: collapse newlines and escape quotes, trim length."""
        if not msg:
            return ''
        s = str(msg)
        # Replace CR/LF with literal backslash-n so the CSV value remains single-line
        s = s.replace('\r\n', '\\n').replace('\r', '\\n').replace('\n', '\\n')
        s = s.replace('"', "'")
        if len(s) > self.max_error_msg_len:
            s = s[:self.max_error_msg_len] + '...'
        return s
        
        # Load existing results if resuming
        
        # Load existing results if resuming
        if self.resume_csv and os.path.exists(self.resume_csv):
            self._load_existing_results()
    
    def _load_existing_results(self):
        """Load existing results from resume CSV file."""
        print(f"\nResuming from: {self.resume_csv}")
        
        # Extract timestamp from filename
        import re
        basename = os.path.basename(self.resume_csv)
        match = re.search(r'(\d{4}-\d{2}-\d{2}_\d{2}:\d{2}:\d{2})', basename)
        if match:
            self.current_datetime_str = match.group(1)
            print(f"Using timestamp: {self.current_datetime_str}")
        
        self.current_results_file = self.resume_csv
        
        with open(self.resume_csv, 'r', newline='') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Backwards compatible: status and error_msg may not exist in older CSVs
                status = row.get('status', '').strip() if 'status' in row else ''
                error_msg = row.get('error_msg', '').strip() if 'error_msg' in row else ''
                self.results.append({
                    'model_name': row['model_name'],
                    'task_type': row['task_type'],
                    'sample_id': int(row['sample_id']),
                    'expected': row['expected'].lower() == 'true',
                    'returned': row['returned'].lower() == 'true',
                    'correct': row['correct'].lower() == 'true',
                    'reasoning_time': float(row['reasoning_time']),
                    'inferences': int(row['inferences'])
                ,
                    'status': status,
                    'error_msg': error_msg
                })
                
                sample_key = (row['model_name'], int(row['sample_id']))
                self.evaluated_samples.add(sample_key)
        
        print(f"Loaded {len(self.results)} existing results")
        print(f"Will skip {len(self.evaluated_samples)} already evaluated samples\n")
    
    def _initialize_results_file(self, suffix=''):
        """Initialize the results CSV file with headers."""
        if not self.current_results_file:
            # Create new file with timestamp
            self.current_datetime_str = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')
            filename = f'evaluation_results{suffix}_{self.current_datetime_str}.csv'
            self.current_results_file = os.path.join(self.output_dir, 'results', filename)
            
            with open(self.current_results_file, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=self.csv_fieldnames)
                writer.writeheader()
            
            print(f"Initialized results file: {self.current_results_file}")
    
    def _append_result_to_file(self, result: Dict):
        """Append a single result to the CSV file immediately."""
        if not self.current_results_file:
            raise RuntimeError("Results file not initialized. Call _initialize_results_file() first.")
        # Sanitize error messages to avoid embedded newlines or problematic characters
        _sanitize = self._sanitize_error_msg
        
        # Ensure all rows written have sanitized error_msg
        raw_err = result.get('error_msg', '')
        # Keep only a sanitized one-line version of the error message in the CSV.
        result['error_msg'] = _sanitize(raw_err)
        with open(self.current_results_file, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=self.csv_fieldnames)
            writer.writerow(result)
    
    def translate_bpmn_model(self, bpmn_path: str, model_name: str) -> str:
        try:            
            model_id = model_name.replace('.bpmn', '')
            eval_model_name = f'{model_id}'
            
            # Measure translation time
            start_time = time.time()
            success, message, translated_path, prolog_code = self.translator.translate_bpmn_file(bpmn_path, eval_model_name)
            translation_time = time.time() - start_time
            
            if not success:
                print(f"  Translation failed: {message}")
                return None
            
            # Collect translation metrics
            if prolog_code:
                program_size_lines = len(prolog_code.split('\n'))
                
                # Count actions (prim_action/exog_action declarations)
                num_prim_actions = len(re.findall(r'^\s*prim_action\s*\(', prolog_code, re.MULTILINE))
                num_exog_actions = len(re.findall(r'^\s*exog_action\s*\(', prolog_code, re.MULTILINE))
                num_actions = num_prim_actions + num_exog_actions
                
                # Count fluents (rel_fluent and fun_fluent declarations)
                num_rel_fluents = len(re.findall(r'^\s*rel_fluent\s*\(', prolog_code, re.MULTILINE))
                num_fun_fluents = len(re.findall(r'^\s*fun_fluent\s*\(', prolog_code, re.MULTILINE))
                num_fluents = num_rel_fluents + num_fun_fluents
                
                # (Do not collect translation metrics here - use the dedicated script.)
            
            return eval_model_name
        except Exception as e:
            print(f"  Error translating {model_name}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_legality_task(self, reasoner: ReasoningService, sample: Dict) -> Optional[ReasoningMetrics]:
        try:
            from reason import parse_action_list
            actions_str = ', '.join(sample['actions'])
            actions = parse_action_list(actions_str)
            success, raw_output = reasoner.reasoner.legality('legality_check', actions)
            metrics = MetricsCollector.parse_prolog_output(raw_output, success)
            return metrics
        
        except Exception as e:
            print(f"    Error in legality task: {e}")
            import traceback
            traceback.print_exc()
            # Return a ReasoningMetrics indicating an error for consistent CSV saving
            return ReasoningMetrics(success=False, reasoning_time=0.0, cpu_time=0.0, inferences=0, result='error', error_msg=str(e))
    
    def run_conformance_task(self, reasoner: ReasoningService, sample: Dict) -> Optional[ReasoningMetrics]:
        try:
            from reason import parse_action_list
            history_str = ', '.join(sample['actions'])
            history_actions = parse_action_list(history_str)
            # Note: actions are already reversed in the sample, so no need to reverse again
            success, raw_output = reasoner.reasoner.conformance_checking(history_actions)
            metrics = MetricsCollector.parse_prolog_output(raw_output, success)
            return metrics
        
        except Exception as e:
            print(f"    Error in conformance task: {e}")
            import traceback
            traceback.print_exc()
            return ReasoningMetrics(success=False, reasoning_time=0.0, cpu_time=0.0, inferences=0, result='error', error_msg=str(e))
    
    def run_projection_task(self, reasoner: ReasoningService, sample: Dict) -> Optional[ReasoningMetrics]:
        try:
            from reason import parse_action_list
            actions_str = ', '.join(sample['actions'])
            actions = parse_action_list(actions_str)
            fluent_name = sample['property']
            # Always query for 'true' to check if the fluent holds
            # The result will be compared with expected_result later
            expected_value = 'true'
            
            success, raw_output = reasoner.reasoner.projection(fluent_name, actions, expected_value)
            metrics = MetricsCollector.parse_prolog_output(raw_output, success)
            return metrics
        
        except Exception as e:
            print(f"    Error in projection task: {e}")
            import traceback
            traceback.print_exc()
            return ReasoningMetrics(success=False, reasoning_time=0.0, cpu_time=0.0, inferences=0, result='error', error_msg=str(e))
    
    def run_verification_task(self, reasoner: ReasoningService, sample: Dict) -> Optional[ReasoningMetrics]:
        try:
            import re
            property_str = sample['property']
            proc_name = 'property_verification'
            
            # Parse property expression (same logic as in reasoning_service.py)
            property_str = re.sub(r'\bnot\s*\(', 'neg(', property_str)
            conditions = []
            current_condition = ""
            paren_depth = 0
            
            for char in property_str:
                if char == '(':
                    paren_depth += 1
                    current_condition += char
                elif char == ')':
                    paren_depth -= 1
                    current_condition += char
                elif char == ',' and paren_depth == 0:
                    if current_condition.strip():
                        conditions.append(current_condition.strip())
                    current_condition = ""
                else:
                    current_condition += char
            
            if current_condition.strip():
                conditions.append(current_condition.strip())
            
            if len(conditions) == 0:
                return None
            elif len(conditions) == 1:
                property_expr = conditions[0]
            else:
                property_expr = conditions[-1]
                for cond in reversed(conditions[:-1]):
                    property_expr = f"and({cond}, {property_expr})"
            
            success, raw_output = reasoner.reasoner.verify_property(property_expr, proc_name)
            metrics = MetricsCollector.parse_prolog_output(raw_output, success)
            return metrics
        
        except Exception as e:
            print(f"    Error in verification task: {e}")
            import traceback
            traceback.print_exc()
            return ReasoningMetrics(success=False, reasoning_time=0.0, cpu_time=0.0, inferences=0, result='error', error_msg=str(e))

    
    def load_samples_from_csv(self, csv_path: str) -> Dict[str, List[Dict]]:
        samples_by_model = {}
        with open(csv_path, 'r', newline='') as f:
            reader = csv.DictReader(f)
            for row in reader:
                task_type = row['task_type']
                if task_type not in ['legality', 'conformance', 'projection', 'verification']:
                    continue    
                
                # Parse actions (empty for verification tasks)
                actions = [a.strip() for a in row['actions'].split(',') if a.strip()]
                expected_result = row['expected_result'].lower() == 'true'
                
                sample = {
                    'task_type': task_type,
                    'sample_id': int(row['sample_id']),
                    'actions': actions,
                    'expected_result': expected_result,
                    'model_name': row['model_name']
                }
                
                # Add property field for projection and verification tasks
                if task_type in ['projection', 'verification']:
                    sample['property'] = row.get('property', '').strip()
                
                model_name = row['model_name']
                if model_name not in samples_by_model:
                    samples_by_model[model_name] = []
                samples_by_model[model_name].append(sample)
        
        return samples_by_model
    
    def evaluate_model(self, model_name: str, samples: List[Dict]):
        """Evaluate a single BPMN model with its samples.
        
        Args:
            model_name: Name of the model (e.g., "process_1.bpmn")
            samples: List of sample dictionaries for this model
        """
        print(f"\n{'='*70}")
        print(f"Evaluating: {model_name}")
        print(f"{'='*70}")
        
        bpmn_file = os.path.join(self.dataset_dir, model_name)
        if not os.path.exists(bpmn_file):
            print(f"  Error: BPMN file not found: {bpmn_file}")
            return
        
        model_id = self.translate_bpmn_model(bpmn_file, model_name)
        if not model_id:
            print(f"  Error: Translation failed for {model_name}")
            return
        print(f"  Translation successful: {model_id}")
        try:
            # Initialize reasoner with evaluation_temp directory
            reasoner = ReasoningService(model_id, model_base_dir=self.pl_models_dir)
            print(f"  Reasoner initialized")
        except Exception as e:
            print(f"  Error initializing reasoner: {e}")
            return
        
        correct = 0
        total = 0
        for sample in samples:
            sample_key = (model_name, sample['sample_id'])
            if sample_key in self.evaluated_samples:
                print(f"\n  Sample {sample['sample_id']} - SKIPPED (already evaluated)")
                continue
            
            total += 1
            task_type = sample['task_type']
            print(f"\n  Sample {sample['sample_id']} ({task_type}):")
            
            if task_type in ['projection', 'verification']:
                print(f"    Property: {sample.get('property', 'N/A')}")
                if sample['actions']:
                    print(f"    Actions: {' -> '.join(sample['actions'][:5])}{'...' if len(sample['actions']) > 5 else ''}")
            else:
                print(f"    Actions: {' -> '.join(sample['actions'][:5])}{'...' if len(sample['actions']) > 5 else ''}")
            
            print(f"    Expected: {sample['expected_result']}")
            
            if task_type == 'legality':
                metrics = self.run_legality_task(reasoner, sample)
            elif task_type == 'conformance':
                metrics = self.run_conformance_task(reasoner, sample)
            elif task_type == 'projection':
                metrics = self.run_projection_task(reasoner, sample)
            elif task_type == 'verification':
                metrics = self.run_verification_task(reasoner, sample)
            else:
                print(f"    Unknown task type: {task_type}")
                continue

            if metrics:
                # Map result to boolean based on task type:
                # - legality/projection/verification: success = True, failure = False
                # - conformance: conforms = True, not_conforms = False
                if task_type == 'conformance':
                    actual_result = metrics.result == 'conforms'
                else:
                    actual_result = metrics.result == 'success'
                
                is_correct = actual_result == sample['expected_result']
                
                if is_correct:
                    correct += 1
                    print(f"    Correct (result: {metrics.result})")
                else:
                    print(f"    Incorrect (result: {metrics.result}, expected: {sample['expected_result']})")
                
                # Store result
                result = {
                    'model_name': model_name,
                    'task_type': task_type,
                    'sample_id': sample['sample_id'],
                    'expected': sample['expected_result'],
                    'returned': actual_result,
                    'correct': is_correct,
                    'reasoning_time': metrics.reasoning_time,
                    'inferences': metrics.inferences
                ,
                    'status': getattr(metrics, 'result', ''),
                    'error_msg': getattr(metrics, 'error_msg', '') or ''
                }
                # Print special messages for timeouts/errors
                status = getattr(metrics, 'result', '')
                if status == 'timeout':
                    print(f"    TIMEOUT detected: {getattr(metrics, 'error_msg', '')}")
                elif status == 'error':
                    print(f"    ERROR detected: {getattr(metrics, 'error_msg', '')}")

                status = getattr(metrics, 'result', '')
                if status == 'timeout':
                    print(f"    TIMEOUT detected: {getattr(metrics, 'error_msg', '')}")
                elif status == 'error':
                    print(f"    ERROR detected: {getattr(metrics, 'error_msg', '')}")

                self.results.append(result)
                
                self._append_result_to_file(result)
            else:
                # If metrics is None or task produced no metrics, record as error
                print(f"    Task failed (no metrics)")
                result = {
                    'model_name': model_name,
                    'task_type': task_type,
                    'sample_id': sample['sample_id'],
                    'expected': sample['expected_result'],
                    'returned': False,
                    'correct': False,
                    'reasoning_time': 0.0,
                    'inferences': 0,
                    'status': 'error',
                    'error_msg': 'Task failed (no metrics)'
                }
                self.results.append(result)
                self._append_result_to_file(result)
        
        accuracy = (correct / total * 100) if total > 0 else 0
        print(f"\n  Model Accuracy: {correct}/{total} ({accuracy:.1f}%)")
    
    def run_evaluation(self):
        print("\n" + "="*70)
        print("EVALUATION - LEGALITY & CONFORMANCE")
        print("="*70)
        
        self._initialize_results_file('_leg_conf')
        
        csv_path = os.path.join(self.output_dir, 'datasets', 'samples_leg_conf.csv')
        print(f"\nLoading samples from: {csv_path}")
        samples_by_model = self.load_samples_from_csv(csv_path)
        print(f"Loaded {sum(len(s) for s in samples_by_model.values())} samples for {len(samples_by_model)} models")

        for model_name, samples in sorted(samples_by_model.items()):
            self.evaluate_model(model_name, samples)
        self.save_results('_leg_conf')
    
    def run_projection_verification_evaluation(self):
        print("\n" + "="*70)
        print("EVALUATION - PROJECTION & PROPERTY VERIFICATION")
        print("="*70)
        
        self._initialize_results_file('_proj_verif')
        
        csv_path = os.path.join(self.output_dir, 'datasets', 'samples_proj_verif.csv')
        print(f"\nLoading samples from: {csv_path}")
        samples_by_model = self.load_samples_from_csv(csv_path)
        print(f"Loaded {sum(len(s) for s in samples_by_model.values())} samples for {len(samples_by_model)} models")

        exams_dataset_dir = os.path.join(self.project_root, 'bpmn', 'exams-bpmn')
        
        for model_name, samples in sorted(samples_by_model.items()):
            self.evaluate_model_with_dataset(model_name, samples, exams_dataset_dir)
        
        self.save_results('_proj_verif')
    
    def evaluate_model_with_dataset(self, model_name: str, samples: List[Dict], dataset_dir: str):
        """Evaluate a single BPMN model with its samples from a specific dataset directory.
        
        Args:
            model_name: Name of the model (e.g., "process_0.bpmn")
            samples: List of sample dictionaries for this model
            dataset_dir: Directory where the BPMN file is located
        """
        print(f"\n{'='*70}")
        print(f"Evaluating: {model_name}")
        print(f"{'='*70}")
        
        bpmn_file = os.path.join(dataset_dir, model_name)
        if not os.path.exists(bpmn_file):
            print(f"  Error: BPMN file not found: {bpmn_file}")
            return
        
        model_id = self.translate_bpmn_model(bpmn_file, model_name)
        if not model_id:
            print(f"  Error: Translation failed for {model_name}")
            return
        print(f"  Translation successful: {model_id}")
        
        try:
            # Initialize reasoner with evaluation_temp directory
            reasoner = ReasoningService(model_id, model_base_dir=self.pl_models_dir)
            print(f"  Reasoner initialized")
        except Exception as e:
            print(f"  Error initializing reasoner: {e}")
            return
        
        correct = 0
        total = 0
        for sample in samples:
            sample_key = (model_name, sample['sample_id'])
            if sample_key in self.evaluated_samples:
                print(f"\n  Sample {sample['sample_id']} - SKIPPED (already evaluated)")
                continue
            
            total += 1
            task_type = sample['task_type']
            print(f"\n  Sample {sample['sample_id']} ({task_type}):")
            
            if task_type in ['projection', 'verification']:
                print(f"    Property: {sample.get('property', 'N/A')}")
                if sample['actions']:
                    print(f"    Actions: {' -> '.join(sample['actions'][:5])}{'...' if len(sample['actions']) > 5 else ''}")
            else:
                print(f"    Actions: {' -> '.join(sample['actions'][:5])}{'...' if len(sample['actions']) > 5 else ''}")
            
            print(f"    Expected: {sample['expected_result']}")
            
            if task_type == 'legality':
                metrics = self.run_legality_task(reasoner, sample)
            elif task_type == 'conformance':
                metrics = self.run_conformance_task(reasoner, sample)
            elif task_type == 'projection':
                metrics = self.run_projection_task(reasoner, sample)
            elif task_type == 'verification':
                metrics = self.run_verification_task(reasoner, sample)
            else:
                print(f"    Unknown task type: {task_type}")
                continue

            if metrics:
                # Map result to boolean based on task type:
                # - legality/projection/verification: success = True, failure = False
                # - conformance: conforms = True, not_conforms = False
                if task_type == 'conformance':
                    actual_result = metrics.result == 'conforms'
                else:
                    actual_result = metrics.result == 'success'
                
                is_correct = actual_result == sample['expected_result']
                
                if is_correct:
                    correct += 1
                    print(f"    Correct (result: {metrics.result})")
                else:
                    print(f"    Incorrect (result: {metrics.result}, expected: {sample['expected_result']})")
                
                # Store result
                result = {
                    'model_name': model_name,
                    'task_type': task_type,
                    'sample_id': sample['sample_id'],
                    'expected': sample['expected_result'],
                    'returned': actual_result,
                    'correct': is_correct,
                    'reasoning_time': metrics.reasoning_time,
                    'inferences': metrics.inferences
                ,
                    'status': getattr(metrics, 'result', ''),
                    'error_msg': getattr(metrics, 'error_msg', '') or ''
                }
                self.results.append(result)
                
                self._append_result_to_file(result)
            else:
                print(f"    Task failed (no metrics)")
                result = {
                    'model_name': model_name,
                    'task_type': task_type,
                    'sample_id': sample['sample_id'],
                    'expected': sample['expected_result'],
                    'returned': False,
                    'correct': False,
                    'reasoning_time': 0.0,
                    'inferences': 0,
                    'status': 'error',
                    'error_msg': 'Task failed (no metrics)'
                }
                self.results.append(result)
                self._append_result_to_file(result)
        
        accuracy = (correct / total * 100) if total > 0 else 0
        print(f"\n  Model Accuracy: {correct}/{total} ({accuracy:.1f}%)")
    
    def save_results(self, suffix=''):
        """Save translation metrics and finalize results file.
        
        Note: Individual results are already saved incrementally during evaluation.
        This method primarily handles translation metrics saving and final reporting.
        """
        output_file = self.current_results_file
        print(f"\nResults saved to: {output_file}")
        
        # (Translation metrics are not produced by this script; use the separate translation metrics generator.)
        
        return output_file
    
    def generate_summary_from_csv(self, csv_file_path: str, suffix=''):
        results = []
        with open(csv_file_path, 'r', newline='') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Parse fields
                task_type = row['task_type']
                expected = row['expected'].lower() == 'true'
                returned_field = row.get('returned', '').lower()
                # returned can be empty or missing; interpret empty as False
                returned = True if returned_field == 'true' else False
                status = row.get('status', '').strip()
                error_msg = row.get('error_msg', '').strip()

                # Default: compute correctness from expected vs returned
                computed_correct = (returned == expected)

                # Policy: treat timeouts/errors as incorrect by default
                if status in ['error', 'timeout']:
                    computed_correct = False

                results.append({
                    'task_type': task_type,
                    'expected': expected,
                    'returned': returned,
                    'correct': computed_correct,
                    'reasoning_time': float(row['reasoning_time']),
                    'inferences': int(row['inferences']),
                    'status': status,
                    'error_msg': error_msg
                })
        
        if not results:
            print("\nNo results to summarize.")
            return
        
        # Helper function to calculate precision, recall, F1
        def calculate_metrics(results_subset, treat_timeouts_incorrect=False):
            """Calculate TP, FP, TN, FN, precision, recall, F1-score."""
            tp = 0
            fp = 0
            tn = 0
            fn = 0
            for r in results_subset:
                expected = r['expected']
                returned = r['returned']
                status = r.get('status', '')

                # If requested, treat timeouts/errors as incorrect by flipping returned to not expected
                if treat_timeouts_incorrect and status in ['error', 'timeout']:
                    returned = not expected

                if expected and returned:
                    tp += 1
                elif (not expected) and returned:
                    fp += 1
                elif (not expected) and (not returned):
                    tn += 1
                elif expected and (not returned):
                    fn += 1
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
            accuracy = (tp + tn) / len(results_subset) if len(results_subset) > 0 else 0.0
            
            return {
                'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,
                'precision': precision, 'recall': recall, 'f1': f1_score, 'accuracy': accuracy
            }
        
        overall_metrics = calculate_metrics(results, treat_timeouts_incorrect=True)
        valid_results = [r for r in results if r.get('status') not in ['error', 'timeout']]
        valid_metrics = calculate_metrics(valid_results) if valid_results else None
        total = len(results)
        correct = sum(1 for r in results if r['correct'])
        
        avg_time = sum(r['reasoning_time'] for r in results) / total
        avg_inferences = sum(r['inferences'] for r in results) / total
        
        legality_results = [r for r in results if r['task_type'] == 'legality']
        conformance_results = [r for r in results if r['task_type'] == 'conformance']
        projection_results = [r for r in results if r['task_type'] == 'projection']
        verification_results = [r for r in results if r['task_type'] == 'verification']
        
        legality_metrics = calculate_metrics(legality_results, treat_timeouts_incorrect=True) if legality_results else None
        conformance_metrics = calculate_metrics(conformance_results, treat_timeouts_incorrect=True) if conformance_results else None
        projection_metrics = calculate_metrics(projection_results, treat_timeouts_incorrect=True) if projection_results else None
        verification_metrics = calculate_metrics(verification_results, treat_timeouts_incorrect=True) if verification_results else None
        
        legality_time = sum(r['reasoning_time'] for r in legality_results) / len(legality_results) if legality_results else 0
        conformance_time = sum(r['reasoning_time'] for r in conformance_results) / len(conformance_results) if conformance_results else 0
        projection_time = sum(r['reasoning_time'] for r in projection_results) / len(projection_results) if projection_results else 0
        verification_time = sum(r['reasoning_time'] for r in verification_results) / len(verification_results) if verification_results else 0
        
        legality_inf = sum(r['inferences'] for r in legality_results) / len(legality_results) if legality_results else 0
        conformance_inf = sum(r['inferences'] for r in conformance_results) / len(conformance_results) if conformance_results else 0
        projection_inf = sum(r['inferences'] for r in projection_results) / len(projection_results) if projection_results else 0
        verification_inf = sum(r['inferences'] for r in verification_results) / len(verification_results) if verification_results else 0
        
        summary_lines = []
        summary_lines.append("=" * 70)
        summary_lines.append("EVALUATION SUMMARY")
        summary_lines.append("=" * 70)
        summary_lines.append(f"Total samples: {total}")
        summary_lines.append(f"Correct: {correct}")
        summary_lines.append("")
        summary_lines.append("OVERALL METRICS:")
        summary_lines.append(f"  Accuracy:  {overall_metrics['accuracy']*100:.1f}%")
        summary_lines.append(f"  Precision: {overall_metrics['precision']*100:.1f}%")
        summary_lines.append(f"  Recall:    {overall_metrics['recall']*100:.1f}%")
        summary_lines.append(f"  F1-Score:  {overall_metrics['f1']*100:.1f}%")
        summary_lines.append(f"  Average Reasoning Time: {avg_time:.3f} seconds")
        if valid_metrics:
            summary_lines.append(f"  Valid-only Accuracy: {valid_metrics['accuracy']*100:.1f}%")
        else:
            summary_lines.append("  Valid-only Accuracy: N/A")
        summary_lines.append(f"  Timeouts: {sum(1 for r in results if r.get('status') == 'timeout')}")
        summary_lines.append(f"  Errors: {sum(1 for r in results if r.get('status') == 'error')}")
        summary_lines.append(f"  Average Inferences: {avg_inferences:.0f}")
        summary_lines.append("")
        summary_lines.append("PER-TASK BREAKDOWN:")
        
        if legality_metrics:
            summary_lines.append(f"Legality: {legality_metrics['tp'] + legality_metrics['tn']}/{len(legality_results)} correct ({legality_metrics['accuracy']*100:.1f}%)")
            summary_lines.append(f"  Precision: {legality_metrics['precision']*100:.1f}%, Recall: {legality_metrics['recall']*100:.1f}%, F1: {legality_metrics['f1']*100:.1f}%")
            summary_lines.append(f"  Avg Time: {legality_time:.3f}s, Avg Inferences: {legality_inf:.0f}")
            summary_lines.append(f"  Timeouts: {sum(1 for r in legality_results if r.get('status') == 'timeout')}, Errors: {sum(1 for r in legality_results if r.get('status') == 'error')}")
        
        if conformance_metrics:
            summary_lines.append(f"Conformance: {conformance_metrics['tp'] + conformance_metrics['tn']}/{len(conformance_results)} correct ({conformance_metrics['accuracy']*100:.1f}%)")
            summary_lines.append(f"  Precision: {conformance_metrics['precision']*100:.1f}%, Recall: {conformance_metrics['recall']*100:.1f}%, F1: {conformance_metrics['f1']*100:.1f}%")
            summary_lines.append(f"  Avg Time: {conformance_time:.3f}s, Avg Inferences: {conformance_inf:.0f}")
            summary_lines.append(f"  Timeouts: {sum(1 for r in conformance_results if r.get('status') == 'timeout')}, Errors: {sum(1 for r in conformance_results if r.get('status') == 'error')}")
        
        if projection_metrics:
            summary_lines.append(f"Projection: {projection_metrics['tp'] + projection_metrics['tn']}/{len(projection_results)} correct ({projection_metrics['accuracy']*100:.1f}%)")
            summary_lines.append(f"  Precision: {projection_metrics['precision']*100:.1f}%, Recall: {projection_metrics['recall']*100:.1f}%, F1: {projection_metrics['f1']*100:.1f}%")
            summary_lines.append(f"  Avg Time: {projection_time:.3f}s, Avg Inferences: {projection_inf:.0f}")
            summary_lines.append(f"  Timeouts: {sum(1 for r in projection_results if r.get('status') == 'timeout')}, Errors: {sum(1 for r in projection_results if r.get('status') == 'error')}")
        
        if verification_metrics:
            summary_lines.append(f"Property Verification: {verification_metrics['tp'] + verification_metrics['tn']}/{len(verification_results)} correct ({verification_metrics['accuracy']*100:.1f}%)")
            summary_lines.append(f"  Precision: {verification_metrics['precision']*100:.1f}%, Recall: {verification_metrics['recall']*100:.1f}%, F1: {verification_metrics['f1']*100:.1f}%")
            summary_lines.append(f"  Avg Time: {verification_time:.3f}s, Avg Inferences: {verification_inf:.0f}")
            summary_lines.append(f"  Timeouts: {sum(1 for r in verification_results if r.get('status') == 'timeout')}, Errors: {sum(1 for r in verification_results if r.get('status') == 'error')}")
        
        summary_lines.append("=" * 70)
        
        print("\n" + "\n".join(summary_lines))
        
        if self.current_datetime_str:
            summary_filename = f'evaluation_summary{suffix}_{self.current_datetime_str}.txt'
            summary_output_file = os.path.join(self.output_dir, 'results', summary_filename)
            with open(summary_output_file, 'w') as f:
                f.write("\n".join(summary_lines) + "\n")
            print(f"\nSummary saved to: {summary_output_file}")
    
    def assess_results(self, csv_file_path: str, suffix: str):
        """Assess results from a specific CSV file and generate summary.
        
        Args:
            csv_file_path: Path to the evaluation results CSV file
            suffix: Suffix for the output summary file (e.g., '_leg_conf')
        """
        if not os.path.exists(csv_file_path):
            print(f"Error: CSV file not found: {csv_file_path}")
            return
        
        # Extract timestamp from CSV filename if possible
        basename = os.path.basename(csv_file_path)
        # Try to extract timestamp from filename like: evaluation_results_leg_conf_2025-11-10_09:44:50.csv
        import re
        match = re.search(r'(\d{4}-\d{2}-\d{2}_\d{2}:\d{2}:\d{2})', basename)
        if match:
            self.current_datetime_str = match.group(1)
        else:
            # Use current timestamp if we can't extract it
            self.current_datetime_str = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')
        
        self.generate_summary_from_csv(csv_file_path, suffix)
        
        print("\n" + "="*70)
        print("GENERATING CHARTS")
        print("="*70)
        try:
            chart_gen = ChartGenerator(csv_file_path, self.output_dir)
            chart_gen.generate_all_charts()
        except Exception as e:
            print(f"Error generating charts: {e}")
            import traceback
            traceback.print_exc()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='BPMN Evaluation - Legality, Conformance, Projection, and Property Verification')
    parser.add_argument('--mode', type=str, default='test', 
                       choices=['test', 'assess'],
                       help='Mode: test (run evaluation) or assess (generate summary from existing CSV)')
    parser.add_argument('--task', type=str, default='all', 
                       choices=['legality_conformance', 'projection_verification', 'all'],
                       help='Which evaluation to run/assess: legality_conformance, projection_verification, or all (default: all)')
    parser.add_argument('--csv', type=str, default=None,
                       help='Path to CSV file for assessment mode (optional, will find latest if not provided)')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to incomplete CSV file to resume evaluation from (test mode only)')
    args = parser.parse_args()
    
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    dataset_dir = os.path.join(project_root, 'bpmn', 'dataset', 'processed')
    output_dir = os.path.join(project_root, 'evaluation')
    
    resume_csv = args.resume
    if resume_csv and not os.path.isabs(resume_csv):
        resume_csv = os.path.join(output_dir, 'results', resume_csv)
    
    evaluator = BPMNEvaluator(dataset_dir, output_dir, resume_csv=resume_csv)
    # Translation metrics generation is auto-detected: if translation metrics CSVs already exist we skip regenerating them
    
    if args.mode == 'test':
        if args.task in ['legality_conformance', 'all']:
            evaluator.run_evaluation()
            
        if args.task in ['projection_verification', 'all']:
            if args.task == 'all':
                evaluator.results = []
                evaluator.bpmn_metrics = []
            evaluator.run_projection_verification_evaluation()
    
    elif args.mode == 'assess':
        results_dir = os.path.join(output_dir, 'results')
        
        if args.task in ['legality_conformance', 'all']:
            if args.csv:
                # Normalize CSV path: if not absolute, assume results_dir
                csv_file = args.csv if os.path.isabs(args.csv) else os.path.join(results_dir, args.csv)
                print(f"Assessing CSV (leg_conf): {csv_file}")
                evaluator.assess_results(csv_file, '_leg_conf')
            else:
                import glob
                csv_files = glob.glob(os.path.join(results_dir, 'evaluation_results_leg_conf_*.csv'))
                if not csv_files:
                    print("Error: No legality_conformance CSV files found in results directory")
                else:
                    csv_file = max(csv_files, key=os.path.getmtime)
                    print(f"Assessing latest legality_conformance results: {csv_file}")
                    evaluator.assess_results(csv_file, '_leg_conf')
        
        if args.task in ['projection_verification', 'all']:
            if args.csv and args.task == 'projection_verification':
                # Normalize CSV path: if not absolute, assume results_dir
                csv_file = args.csv if os.path.isabs(args.csv) else os.path.join(results_dir, args.csv)
                print(f"Assessing CSV (proj_verif): {csv_file}")
                evaluator.assess_results(csv_file, '_proj_verif')
            else:
                import glob
                csv_files = glob.glob(os.path.join(results_dir, 'evaluation_results_proj_verif_*.csv'))
                if not csv_files:
                    print("Error: No projection_verification CSV files found in results directory")
                else:
                    csv_file = max(csv_files, key=os.path.getmtime)
                    print(f"Assessing latest projection_verification results: {csv_file}")
                    evaluator.assess_results(csv_file, '_proj_verif')
